{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing a single CT scan in python\n",
    "\n",
    "\n",
    "Arguably, understanding the data we are presented should be the very first step in appreciating the problem at hand. I have decided to play around with files and visualizations before, for the sake of feeling alittle but of achievment, and getting a gut-instinct of what we have to deal with. I will turn my attention to perform more analytical study of a single CT scan, and my aim is to understand the format. I will also use this opportunity to build any additional utilities needed for future Model definition work.\n",
    "\n",
    "I am very intrigued to visualize 1 CT scan in 3D. So far, we have visualized only cross-segments and projections. Let's spend some time to look up resources that will help us achieve visualizing in 3D.\n",
    "\n",
    "- I came across a fantastic blog that will serve as the foundation for our analysis: [Vincente Rodriguez blog](https://vincentblog.xyz/posts/medical-images-in-python-computed-tomography).\n",
    "- The [Wikipedia page](https://en.wikipedia.org/wiki/Marching_cubes) about `Marching cubes` is also a great introduction for 3D visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LUNA16.utils.mermaid import mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding CT scans and Voxels\n",
    "\n",
    "Before we get too far into the project, we need to take a moment to explain what a CT scan is. We have already started exploring the contents of our CT scans and as we progress we will be using data from them extensively for this project, so it is important to have a working understanding of the format we are dealing with. The key aspect we have to know is that CT scans are effectively 3D X-rays, represented as 3D arrays of single-channel data that is concatenated. It is like a stacked array of grayscale images.\n",
    "\n",
    "A [voxel](https://en.wikipedia.org/wiki/Voxel) is a 3 dimensional representation of some value in 3D space.As with pixels in a 2D bitmap, *voxels themselves do not typically have their coordinates explicitly encoded with their values* Instead, they depend on a rendering systems to infer the position of a voxel, and we will revisit this point in a bit.\n",
    "\n",
    "A Voxel is the 3D equivalent of a Pixel. As such, remember that:\n",
    "\n",
    "- A Voxel does not have \"width\" encoded in it, yet it represents a volume - just as how a Pixel does not have a width value, until it is displayed by a renderer on either a screen or transformed to be printed. What this effectively means is that when you will slice a single Voxel from an array, you will retrieve a single point in memory that represents a volumetric point in 3D\n",
    "- Voxels depend on some form of rendering system to infer their positions.\n",
    "\n",
    "Voxels can either be cubic or they can have different shapes. To appreciate further more the data that constitutes this challenge, I provide few notes retrieved from the [LUNA 16 Challenge page](https://luna16.grand-challenge.org/data/):\n",
    "\n",
    "|#|Descriptor | Note |\n",
    "|-|-----------|------|\n",
    "|1| The data used is from the [LIDC/IDRI database](http://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI) | What's this database? we need to research this. |\n",
    "|2| Scans with slice thickness greater than 2.5 mm were excluded | Why were they excluded - is that something that will affect our work?|\n",
    "|3| Radiologist have annotated the data: they have marked 'lesions' as non-nodules, <3 mm nodules, and >3 mm ones | What's a lesion, and how have they annotated it? Read their [publication](http://www.ncbi.nlm.nih.gov/pubmed/21452728). Quote \"Each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories (\"nodule > or =3 mm,\" \"nodule <3 mm,\" and \"non-nodule > or =3 mm\")\"|\n",
    "|4| Annotations | Each finding is on a line of annotations.csv. Each line contains SeriesUID and x,y,z position in **world coordinates**, and corresponding diameter in **mm**. Do we need to perform any transformation to coordinates?\n",
    "|5| Objective | The LUNA16 challenge will focus on a large-scale evaluation of *automatic nodule detection* algorithms. There are two \"tracks\": (1) Complete systems for nodule detection, (2) systems that use a list of locations of possible nodules. The organizers provide this list to also allow teams to participate with an algorithm that only determines the likelihood for a given location in a CT scan to contain a pulmonary nodule.|\n",
    "\n",
    "Some links that I have found valuable are:\n",
    "- [Definition of Lesions](https://en.wikipedia.org/wiki/Lesion) on Wikipedia\n",
    "- [Definition of Nodules](https://en.wikipedia.org/wiki/Nodule_(medicine)) on Wikipedia\n",
    "- The [History of CT scans](https://www.youtube.com/watch?v=M6vsBcxHPZU&t=785s) on Youtube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/Zmxvd2NoYXJ0IFREIAogICAgQVtMSURDLURJUkkgZGF0YXNldF0gLS0+IHxpbmNsdWRlcyAjIDE3NzZ8IEJbQ1Qgc2NhbnNdIAogICAgQSAtLT4gfE5vdCBzdXJlIGhvdyBtYW55IGFyZSBpbmNsdWRlZHxaW0Fubm90YXRpb25zIG9mIGxlc2lvbnNdCiAgICBzdWJncmFwaCBvYmogW09iamVjdGl2ZV0KICAgIFogLS0+IFlbTm9kdWxlID49IDMgbW1dCiAgICBaIC0tPiBYW05vZHVsZSA8PSAzIG1tXQogICAgZW5kIAogICAgWiAtLT4gV1tub24tTm9kdWxlID49IDMgY21tXQogICAgc3ViZ3JhcGggZXhpc3QgW0FscmVhZHkgZXhwbG9yZWRdCiAgICBCIC0tPiBHW1JhdyBmaWxlc10KICAgIGVuZAogICAgc3ViZ3JhcGggY29vcmRyYXcgW2Nvb3JkLl0KICAgIEhbSSxSLENdIAogICAgZW5kCiAgICBzdWJncmFwaCBjb29yZGFubm90IFtjb29yZC5dCiAgICBJW1gsWSxaXQogICAgZW5kCmV4aXN0IC0tPmNvb3JkcmF3ICAgIApvYmogLS0+IGNvb3JkYW5ub3Q=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm.graph(mm.data_explanation_06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ipyvolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LUNA16.utils.analyze_folders import analyze_folder\n",
    "from LUNA16.utils.analyze_data_distribution import read_mhd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from dask.distributed import Client\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"/home/azureuser/cloudfiles/data/LUNA16/extracted\"\n",
    "all_files = analyze_folder(ROOT_FOLDER)\n",
    "assert len(all_files) == 3567\n",
    "all_mhd_files = [file for file in all_files if file.extension == \"mhd\"]\n",
    "assert len(all_mhd_files) == 1776\n",
    "random_uid = random.choice([file.filename for file in all_files if file.extension ==\"mhd\"])\n",
    "notebook_files = [file for file in all_files if file.filename ==random_uid]\n",
    "mhd = [file for file in notebook_files if file.extension == \"mhd\"]\n",
    "mhd_image = sitk.ReadImage(mhd[0].folder)\n",
    "mhd_image = np.array(sitk.GetArrayFromImage(mhd_image), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[34. 38. 54.]\n",
      "  [47. 61. 54.]\n",
      "  [23. 26. 29.]]\n",
      "\n",
      " [[21. 51. 57.]\n",
      "  [20. 42. 34.]\n",
      "  [30. 31. 12.]]\n",
      "\n",
      " [[36. 45. 32.]\n",
      "  [20. 26. 12.]\n",
      "  [33. 47. 17.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ipyvolume as ipv\n",
    "# V = np.zeros((128,128,128)) # our 3d array\n",
    "# # outer box\n",
    "# V[30:-30,30:-30,30:-30] = 0.75\n",
    "# V[35:-35,35:-35,35:-35] = 0.0\n",
    "# # inner box\n",
    "# V[50:-50,50:-50,50:-50] = 0.25\n",
    "# V[55:-55,55:-55,55:-55] = 0.0\n",
    "V = np.clip(mhd_image, -500, 500)\n",
    "middle = np.round(V.shape[0] /2)\n",
    "middle = int(middle)\n",
    "\n",
    "small_V = V[middle -1: middle + 2, 254:257, 254:257]\n",
    "print(small_V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/numpy/linalg/linalg.py:2514: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = asarray(x)\n",
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/ipyvolume/serialize.py:81: RuntimeWarning: invalid value encountered in true_divide\n",
      "  gradient = gradient / np.sqrt(gradient[0]**2 + gradient[1]**2 + gradient[2]**2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd2553b99a74ff890f1d3d0b99d3fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HBox(children=(Label(value='levels:'), FloatSlider(value=0.1, max=1.0, step=0.00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd2553b99a74ff890f1d3d0b99d3fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HBox(children=(Label(value='levels:'), FloatSlider(value=0.1, max=1.0, step=0.00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipv.figure()\n",
    "#ipv.volshow(V, level=[-1000, 1000], opacity=0.03, level_width=0.1, data_min=-1000, data_max=1000)\n",
    "#ipv.volshow(small_V, opacity=0.03, level_width=0.1, level=[-1, 1], data_min=-1, data_max=1)\n",
    "ipv.examples.ball(rmax=3, rmin=0, shape=128, limits=[-4, 4], draw=True, show=True)\n",
    "ipv.view(-30, 40)\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ee6c66f222b9dd854c6b84f7c12e26b879f4083bb72ef0f1675b6e77ebaebc0"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "azureml_py38_pt_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
